{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers in Artificial Neural Networks (ANNs)\n",
    "\n",
    "In an ANN, data flows sequentially from the input layer through one or more hidden layers to the output layer. Each layer comprises neurons that process input and pass the results onward. Below are the common types of hidden layers, their roles, and their uses:\n",
    "\n",
    "#### 1. **Base Class for Layers**\n",
    "- **Layer**: Abstract base class for all layers. It defines the structure for forward and backward passes and parameter handling.\n",
    "\n",
    "#### 2. **Core Layer Types**\n",
    "- **Dense**: Fully connected layer that applies matrix multiplication with weights and biases, followed by an activation function to introduce non-linearity.\n",
    "\n",
    "#### 3. **Recurrent Layers**\n",
    "- **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)**: Layers used in Recurrent Neural Networks (RNNs) for sequence data such as time series or language. They maintain information across time steps, capturing temporal dependencies and context.\n",
    "\n",
    "#### 4. **Convolutional Layers**\n",
    "- **Conv2D**: Convolution layer designed for image data. It applies filters to input to capture spatial features like edges and textures, producing feature maps.\n",
    "\n",
    "#### 5. **Regularization Layers**\n",
    "- **Dropout**: Randomly drops neurons during training to prevent overfitting by reducing the network's reliance on specific neurons.\n",
    "\n",
    "#### 6. **Dimensionality Reduction Layers**\n",
    "- **Pooling Layers**: Reduce spatial dimensions while retaining important features. \n",
    "  - **MaxPooling2D**: Retains the maximum value from a region.\n",
    "  - **AveragePooling2D**: Computes the average value from a region.\n",
    "\n",
    "#### 7. **Normalization Layers**\n",
    "- **BatchNormalization**: Normalizes inputs by subtracting the batch mean and dividing by the standard deviation. This stabilizes and speeds up training.\n",
    "\n",
    "#### 8. **Reshaping and Scaling Layers**\n",
    "- **Flatten**: Converts multi-dimensional data into a 2D format, typically for input into dense layers.\n",
    "- **UpSampling2D**: Upscales data by repeating rows and columns for tasks requiring higher resolution.\n",
    "- **Reshape**: Adjusts the shape of tensors to fit the input requirements of subsequent layers.\n",
    "\n",
    "#### 9. **Padding Layers**\n",
    "- **ZeroPadding2D** and **ConstantPadding2D**: Add padding to inputs to control spatial dimensions, allowing for consistent output shapes or emphasizing specific features.\n",
    "\n",
    "These hidden layers form the building blocks of ANNs, allowing them to model complex data patterns and relationships across diverse applications like vision, language, and time-series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kennethleungty/Neural-Network-Architecture-Diagrams for cool diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import nbimporter\n",
    "from activation_functions import Sigmoid, ReLU, LeakyReLU, TanH, ELU, Softmax\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    def set_input_shape(self, shape):\n",
    "        \"\"\" Sets the shape that the layer expects of the input in the forward pass method \"\"\"\n",
    "        self.input_shape = shape\n",
    "    def layer_name(self):\n",
    "        \"\"\" The name of the layer. Used in model summary. \"\"\"\n",
    "        return self.__class__.__name__\n",
    "    def parameters(self):\n",
    "        \"\"\" The number of trainable parameters used by the layer \"\"\"\n",
    "        return 0\n",
    "    def forward_pass(self, X, training):\n",
    "        \"\"\" Propogates the signal forward in the network \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    def backward_pass(self, accum_grad):\n",
    "        \"\"\" Propogates the accumulated gradient backwards in the network.\n",
    "        If the has trainable weights then these weights are also tuned in this method.\n",
    "        As input (accum_grad) it receives the gradient with respect to the output of the layer and\n",
    "        returns the gradient with respect to the output of the previous layer. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    def output_shape(self):\n",
    "        \"\"\" The shape of the output produced by forward_pass \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"A fully-connected NN layer.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_units(int): The number of neurons in the layer.\n",
    "    input_shape(tuple): The expected input shape of the layer. For dense layers a single digit specifying\n",
    "        the number of features of the input. Must be specified if it is the first layer in the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, input_shape=None):\n",
    "        self.layer_input = None\n",
    "        self.input_shape = input_shape\n",
    "        self.n_units = n_units\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.w0 = None\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        # Initialize the weights\n",
    "        limit = 1 / math.sqrt(self.input_shape[0])\n",
    "        self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
    "        self.w0 = np.zeros((1, self.n_units))\n",
    "        # Weight optimizers\n",
    "        self.W_opt  = copy.copy(optimizer)\n",
    "        self.w0_opt = copy.copy(optimizer)\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return X.dot(self.W) + self.w0\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        # Save weights used during forwards pass\n",
    "        W = self.W\n",
    "        if self.trainable:\n",
    "            # Calculate gradient w.r.t layer weights\n",
    "            grad_w = self.layer_input.T.dot(accum_grad)\n",
    "            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n",
    "            # Update the layer weights\n",
    "            self.W = self.W_opt.update(self.W, grad_w)\n",
    "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
    "        # Return accumulated gradient for next layer\n",
    "        # Calculated based on the weights used during the forward pass\n",
    "        accum_grad = accum_grad.dot(W.T)\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        return (self.n_units, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(Layer):\n",
    "    \"\"\"A Vanilla Fully-Connected Recurrent Neural Network layer.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_units: int: The number of hidden states in the layer.\n",
    "    activation: string: The name of the activation function which will be applied to the output of each state.\n",
    "    bptt_trunc: int: Decides how many time steps the gradient should be propagated backwards through states\n",
    "        given the loss gradient for time step t.\n",
    "    input_shape: tuple: The expected input shape of the layer. For dense layers a single digit specifying\n",
    "        the number of features of the input. Must be specified if it is the first layer in\n",
    "        the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, activation='tanh', bptt_trunc=5, input_shape=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.n_units = n_units\n",
    "        self.activation = activation_functions[activation]()\n",
    "        self.trainable = True\n",
    "        self.bptt_trunc = bptt_trunc\n",
    "        self.W = None # Weight of the previous state\n",
    "        self.V = None # Weight of the output\n",
    "        self.U = None # Weight of the input\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        timesteps, input_dim = self.input_shape\n",
    "        # Initialize the weights\n",
    "        limit = 1 / math.sqrt(input_dim)\n",
    "        self.U  = np.random.uniform(-limit, limit, (self.n_units, input_dim))\n",
    "        limit = 1 / math.sqrt(self.n_units)\n",
    "        self.V = np.random.uniform(-limit, limit, (input_dim, self.n_units))\n",
    "        self.W  = np.random.uniform(-limit, limit, (self.n_units, self.n_units))\n",
    "        # Weight optimizers\n",
    "        self.U_opt  = copy.copy(optimizer)\n",
    "        self.V_opt = copy.copy(optimizer)\n",
    "        self.W_opt = copy.copy(optimizer)\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.prod(self.W.shape) + np.prod(self.U.shape) + np.prod(self.V.shape)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        batch_size, timesteps, input_dim = X.shape\n",
    "\n",
    "        # Save these values for use in backprop.\n",
    "        self.state_input = np.zeros((batch_size, timesteps, self.n_units))\n",
    "        self.states = np.zeros((batch_size, timesteps+1, self.n_units))\n",
    "        self.outputs = np.zeros((batch_size, timesteps, input_dim))\n",
    "\n",
    "        # Set last time step to zero for calculation of the state_input at time step zero\n",
    "        self.states[:, -1] = np.zeros((batch_size, self.n_units))\n",
    "        for t in range(timesteps):\n",
    "            # Input to state_t is the current input and output of previous states\n",
    "            self.state_input[:, t] = X[:, t].dot(self.U.T) + self.states[:, t-1].dot(self.W.T)\n",
    "            self.states[:, t] = self.activation(self.state_input[:, t])\n",
    "            self.outputs[:, t] = self.states[:, t].dot(self.V.T)\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        _, timesteps, _ = accum_grad.shape\n",
    "\n",
    "        # Variables where we save the accumulated gradient w.r.t each parameter\n",
    "        grad_U = np.zeros_like(self.U)\n",
    "        grad_V = np.zeros_like(self.V)\n",
    "        grad_W = np.zeros_like(self.W)\n",
    "        # The gradient w.r.t the layer input.\n",
    "        # Will be passed on to the previous layer in the network\n",
    "        accum_grad_next = np.zeros_like(accum_grad)\n",
    "\n",
    "        # Back Propagation Through Time\n",
    "        for t in reversed(range(timesteps)):\n",
    "            # Update gradient w.r.t V at time step t\n",
    "            grad_V += accum_grad[:, t].T.dot(self.states[:, t])\n",
    "            # Calculate the gradient w.r.t the state input\n",
    "            grad_wrt_state = accum_grad[:, t].dot(self.V) * self.activation.gradient(self.state_input[:, t])\n",
    "            # Gradient w.r.t the layer input\n",
    "            accum_grad_next[:, t] = grad_wrt_state.dot(self.U)\n",
    "            # Update gradient w.r.t W and U by backprop. from time step t for at most\n",
    "            # self.bptt_trunc number of time steps\n",
    "            for t_ in reversed(np.arange(max(0, t - self.bptt_trunc), t+1)):\n",
    "                grad_U += grad_wrt_state.T.dot(self.layer_input[:, t_])\n",
    "                grad_W += grad_wrt_state.T.dot(self.states[:, t_-1])\n",
    "                # Calculate gradient w.r.t previous state\n",
    "                grad_wrt_state = grad_wrt_state.dot(self.W) * self.activation.gradient(self.state_input[:, t_-1])\n",
    "\n",
    "        # Update weights\n",
    "        self.U = self.U_opt.update(self.U, grad_U)\n",
    "        self.V = self.V_opt.update(self.V, grad_V)\n",
    "        self.W = self.W_opt.update(self.W, grad_W)\n",
    "\n",
    "        return accum_grad_next\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    \"\"\"A 2D Convolution Layer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_filters: int\n",
    "        The number of filters that will convolve over the input matrix. The number of channels\n",
    "        of the output shape.\n",
    "    filter_shape: tuple\n",
    "        A tuple (filter_height, filter_width).\n",
    "    input_shape: tuple\n",
    "        The shape of the expected input of the layer. (batch_size, channels, height, width)\n",
    "        Only needs to be specified for first layer in the network.\n",
    "    padding: string\n",
    "        Either 'same' or 'valid'. 'same' results in padding being added so that the output height and width\n",
    "        matches the input height and width. For 'valid' no padding is added.\n",
    "    stride: int\n",
    "        The stride length of the filters during the convolution over the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filters, filter_shape, input_shape=None, padding='same', stride=1):\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_shape = filter_shape\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.input_shape = input_shape\n",
    "        self.trainable = True\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        # Initialize the weights\n",
    "        filter_height, filter_width = self.filter_shape\n",
    "        channels = self.input_shape[0]\n",
    "        limit = 1 / math.sqrt(np.prod(self.filter_shape))\n",
    "        self.W  = np.random.uniform(-limit, limit, size=(self.n_filters, channels, filter_height, filter_width))\n",
    "        self.w0 = np.zeros((self.n_filters, 1))\n",
    "        # Weight optimizers\n",
    "        self.W_opt  = copy.copy(optimizer)\n",
    "        self.w0_opt = copy.copy(optimizer)\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        batch_size, channels, height, width = X.shape\n",
    "        self.layer_input = X\n",
    "        # Turn image shape into column shape\n",
    "        # (enables dot product between input and weights)\n",
    "        self.X_col = image_to_column(X, self.filter_shape, stride=self.stride, output_shape=self.padding)\n",
    "        # Turn weights into column shape\n",
    "        self.W_col = self.W.reshape((self.n_filters, -1))\n",
    "        # Calculate output\n",
    "        output = self.W_col.dot(self.X_col) + self.w0\n",
    "        # Reshape into (n_filters, out_height, out_width, batch_size)\n",
    "        output = output.reshape(self.output_shape() + (batch_size, ))\n",
    "        # Redistribute axises so that batch size comes first\n",
    "        return output.transpose(3,0,1,2)\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        # Reshape accumulated gradient into column shape\n",
    "        accum_grad = accum_grad.transpose(1, 2, 3, 0).reshape(self.n_filters, -1)\n",
    "\n",
    "        if self.trainable:\n",
    "            # Take dot product between column shaped accum. gradient and column shape\n",
    "            # layer input to determine the gradient at the layer with respect to layer weights\n",
    "            grad_w = accum_grad.dot(self.X_col.T).reshape(self.W.shape)\n",
    "            # The gradient with respect to bias terms is the sum similarly to in Dense layer\n",
    "            grad_w0 = np.sum(accum_grad, axis=1, keepdims=True)\n",
    "\n",
    "            # Update the layers weights\n",
    "            self.W = self.W_opt.update(self.W, grad_w)\n",
    "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
    "\n",
    "        # Recalculate the gradient which will be propogated back to prev. layer\n",
    "        accum_grad = self.W_col.T.dot(accum_grad)\n",
    "        # Reshape from column shape to image shape\n",
    "        accum_grad = column_to_image(accum_grad,\n",
    "                                self.layer_input.shape,\n",
    "                                self.filter_shape,\n",
    "                                stride=self.stride,\n",
    "                                output_shape=self.padding)\n",
    "\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        channels, height, width = self.input_shape\n",
    "        pad_h, pad_w = determine_padding(self.filter_shape, output_shape=self.padding)\n",
    "        output_height = (height + np.sum(pad_h) - self.filter_shape[0]) / self.stride + 1\n",
    "        output_width = (width + np.sum(pad_w) - self.filter_shape[1]) / self.stride + 1\n",
    "        return self.n_filters, int(output_height), int(output_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    \"\"\"Batch normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.trainable = True\n",
    "        self.eps = 0.01\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "\n",
    "    def initialize(self, optimizer):\n",
    "        # Initialize the parameters\n",
    "        self.gamma  = np.ones(self.input_shape)\n",
    "        self.beta = np.zeros(self.input_shape)\n",
    "        # parameter optimizers\n",
    "        self.gamma_opt  = copy.copy(optimizer)\n",
    "        self.beta_opt = copy.copy(optimizer)\n",
    "\n",
    "    def parameters(self):\n",
    "        return np.prod(self.gamma.shape) + np.prod(self.beta.shape)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "\n",
    "        # Initialize running mean and variance if first run\n",
    "        if self.running_mean is None:\n",
    "            self.running_mean = np.mean(X, axis=0)\n",
    "            self.running_var = np.var(X, axis=0)\n",
    "\n",
    "        if training and self.trainable:\n",
    "            mean = np.mean(X, axis=0)\n",
    "            var = np.var(X, axis=0)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        # Statistics saved for backward pass\n",
    "        self.X_centered = X - mean\n",
    "        self.stddev_inv = 1 / np.sqrt(var + self.eps)\n",
    "\n",
    "        X_norm = self.X_centered * self.stddev_inv\n",
    "        output = self.gamma * X_norm + self.beta\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "\n",
    "        # Save parameters used during the forward pass\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # If the layer is trainable the parameters are updated\n",
    "        if self.trainable:\n",
    "            X_norm = self.X_centered * self.stddev_inv\n",
    "            grad_gamma = np.sum(accum_grad * X_norm, axis=0)\n",
    "            grad_beta = np.sum(accum_grad, axis=0)\n",
    "\n",
    "            self.gamma = self.gamma_opt.update(self.gamma, grad_gamma)\n",
    "            self.beta = self.beta_opt.update(self.beta, grad_beta)\n",
    "\n",
    "        batch_size = accum_grad.shape[0]\n",
    "\n",
    "        # The gradient of the loss with respect to the layer inputs (use weights and statistics from forward pass)\n",
    "        accum_grad = (1 / batch_size) * gamma * self.stddev_inv * (\n",
    "            batch_size * accum_grad\n",
    "            - np.sum(accum_grad, axis=0)\n",
    "            - self.X_centered * self.stddev_inv**2 * np.sum(accum_grad * self.X_centered, axis=0)\n",
    "            )\n",
    "\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.input_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PoolingLayer(Layer):\n",
    "    \"\"\"A parent class of MaxPooling2D and AveragePooling2D\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_shape=(2, 2), stride=1, padding=0):\n",
    "        self.pool_shape = pool_shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.trainable = True\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "\n",
    "        batch_size, channels, height, width = X.shape\n",
    "\n",
    "        _, out_height, out_width = self.output_shape()\n",
    "\n",
    "        X = X.reshape(batch_size*channels, 1, height, width)\n",
    "        X_col = image_to_column(X, self.pool_shape, self.stride, self.padding)\n",
    "\n",
    "        # MaxPool or AveragePool specific method\n",
    "        output = self._pool_forward(X_col)\n",
    "\n",
    "        output = output.reshape(out_height, out_width, batch_size, channels)\n",
    "        output = output.transpose(2, 3, 0, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        batch_size, _, _, _ = accum_grad.shape\n",
    "        channels, height, width = self.input_shape\n",
    "        accum_grad = accum_grad.transpose(2, 3, 0, 1).ravel()\n",
    "\n",
    "        # MaxPool or AveragePool specific method\n",
    "        accum_grad_col = self._pool_backward(accum_grad)\n",
    "\n",
    "        accum_grad = column_to_image(accum_grad_col, (batch_size * channels, 1, height, width), self.pool_shape, self.stride, 0)\n",
    "        accum_grad = accum_grad.reshape((batch_size,) + self.input_shape)\n",
    "\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        channels, height, width = self.input_shape\n",
    "        out_height = (height - self.pool_shape[0]) / self.stride + 1\n",
    "        out_width = (width - self.pool_shape[1]) / self.stride + 1\n",
    "        assert out_height % 1 == 0\n",
    "        assert out_width % 1 == 0\n",
    "        return channels, int(out_height), int(out_width)\n",
    "\n",
    "\n",
    "class MaxPooling2D(PoolingLayer):\n",
    "    def _pool_forward(self, X_col):\n",
    "        arg_max = np.argmax(X_col, axis=0).flatten()\n",
    "        output = X_col[arg_max, range(arg_max.size)]\n",
    "        self.cache = arg_max\n",
    "        return output\n",
    "\n",
    "    def _pool_backward(self, accum_grad):\n",
    "        accum_grad_col = np.zeros((np.prod(self.pool_shape), accum_grad.size))\n",
    "        arg_max = self.cache\n",
    "        accum_grad_col[arg_max, range(accum_grad.size)] = accum_grad\n",
    "        return accum_grad_col\n",
    "\n",
    "class AveragePooling2D(PoolingLayer):\n",
    "    def _pool_forward(self, X_col):\n",
    "        output = np.mean(X_col, axis=0)\n",
    "        return output\n",
    "\n",
    "    def _pool_backward(self, accum_grad):\n",
    "        accum_grad_col = np.zeros((np.prod(self.pool_shape), accum_grad.size))\n",
    "        accum_grad_col[:, range(accum_grad.size)] = 1. / accum_grad_col.shape[0] * accum_grad\n",
    "        return accum_grad_col\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConstantPadding2D(Layer):\n",
    "    \"\"\"Adds rows and columns of constant values to the input.\n",
    "    Expects the input to be of shape (batch_size, channels, height, width)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    padding: tuple\n",
    "        The amount of padding along the height and width dimension of the input.\n",
    "        If (pad_h, pad_w) the same symmetric padding is applied along height and width dimension.\n",
    "        If ((pad_h0, pad_h1), (pad_w0, pad_w1)) the specified padding is added to beginning and end of\n",
    "        the height and width dimension.\n",
    "    padding_value: int or tuple\n",
    "        The value the is added as padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, padding, padding_value=0):\n",
    "        self.padding = padding\n",
    "        self.trainable = True\n",
    "        if not isinstance(padding[0], tuple):\n",
    "            self.padding = ((padding[0], padding[0]), padding[1])\n",
    "        if not isinstance(padding[1], tuple):\n",
    "            self.padding = (self.padding[0], (padding[1], padding[1]))\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        output = np.pad(X,\n",
    "            pad_width=((0,0), (0,0), self.padding[0], self.padding[1]),\n",
    "            mode=\"constant\",\n",
    "            constant_values=self.padding_value)\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        pad_top, pad_left = self.padding[0][0], self.padding[1][0]\n",
    "        height, width = self.input_shape[1], self.input_shape[2]\n",
    "        accum_grad = accum_grad[:, :, pad_top:pad_top+height, pad_left:pad_left+width]\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        new_height = self.input_shape[1] + np.sum(self.padding[0])\n",
    "        new_width = self.input_shape[2] + np.sum(self.padding[1])\n",
    "        return (self.input_shape[0], new_height, new_width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ZeroPadding2D(ConstantPadding2D):\n",
    "    \"\"\"Adds rows and columns of zero values to the input.\n",
    "    Expects the input to be of shape (batch_size, channels, height, width)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    padding: tuple\n",
    "        The amount of padding along the height and width dimension of the input.\n",
    "        If (pad_h, pad_w) the same symmetric padding is applied along height and width dimension.\n",
    "        If ((pad_h0, pad_h1), (pad_w0, pad_w1)) the specified padding is added to beginning and end of\n",
    "        the height and width dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, padding):\n",
    "        self.padding = padding\n",
    "        if isinstance(padding[0], int):\n",
    "            self.padding = ((padding[0], padding[0]), padding[1])\n",
    "        if isinstance(padding[1], int):\n",
    "            self.padding = (self.padding[0], (padding[1], padding[1]))\n",
    "        self.padding_value = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\" Turns a multidimensional matrix into two-dimensional \"\"\"\n",
    "    def __init__(self, input_shape=None):\n",
    "        self.prev_shape = None\n",
    "        self.trainable = True\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.prev_shape = X.shape\n",
    "        return X.reshape((X.shape[0], -1))\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        return accum_grad.reshape(self.prev_shape)\n",
    "\n",
    "    def output_shape(self):\n",
    "        return (np.prod(self.input_shape),)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UpSampling2D(Layer):\n",
    "    \"\"\" Nearest neighbor up sampling of the input. Repeats the rows and\n",
    "    columns of the data by size[0] and size[1] respectively.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    size: tuple\n",
    "        (size_y, size_x) - The number of times each axis will be repeated.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(2,2), input_shape=None):\n",
    "        self.prev_shape = None\n",
    "        self.trainable = True\n",
    "        self.size = size\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.prev_shape = X.shape\n",
    "        # Repeat each axis as specified by size\n",
    "        X_new = X.repeat(self.size[0], axis=2).repeat(self.size[1], axis=3)\n",
    "        return X_new\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        # Down sample input to previous shape\n",
    "        accum_grad = accum_grad[:, :, ::self.size[0], ::self.size[1]]\n",
    "        return accum_grad\n",
    "\n",
    "    def output_shape(self):\n",
    "        channels, height, width = self.input_shape\n",
    "        return channels, self.size[0] * height, self.size[1] * width\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reshape(Layer):\n",
    "    \"\"\" Reshapes the input tensor into specified shape\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    shape: tuple\n",
    "        The shape which the input shall be reshaped to.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, input_shape=None):\n",
    "        self.prev_shape = None\n",
    "        self.trainable = True\n",
    "        self.shape = shape\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.prev_shape = X.shape\n",
    "        return X.reshape((X.shape[0], ) + self.shape)\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        return accum_grad.reshape(self.prev_shape)\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dropout(Layer):\n",
    "    \"\"\"A layer that randomly sets a fraction p of the output units of the previous layer\n",
    "    to zero.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    p: float\n",
    "        The probability that unit x is set to zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.2):\n",
    "        self.p = p\n",
    "        self._mask = None\n",
    "        self.input_shape = None\n",
    "        self.n_units = None\n",
    "        self.pass_through = True\n",
    "        self.trainable = True\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        c = (1 - self.p)\n",
    "        if training:\n",
    "            self._mask = np.random.uniform(size=X.shape) > self.p\n",
    "            c = self._mask\n",
    "        return X * c\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        return accum_grad * self._mask\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.input_shape\n",
    "\n",
    "activation_functions = {\n",
    "    'relu': ReLU,\n",
    "    'sigmoid': Sigmoid,\n",
    "    'elu': ELU,\n",
    "    'softmax': Softmax,\n",
    "    'leaky_relu': LeakyReLU,\n",
    "    'tanh': TanH,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Activation(Layer):\n",
    "    \"\"\"A layer that applies an activation operation to the input.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name: string\n",
    "        The name of the activation function that will be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.activation_name = name\n",
    "        self.activation_func = activation_functions[name]()\n",
    "        self.trainable = True\n",
    "\n",
    "    def layer_name(self):\n",
    "        return \"Activation (%s)\" % (self.activation_func.__class__.__name__)\n",
    "\n",
    "    def forward_pass(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return self.activation_func(X)\n",
    "\n",
    "    def backward_pass(self, accum_grad):\n",
    "        return accum_grad * self.activation_func.gradient(self.layer_input)\n",
    "\n",
    "    def output_shape(self):\n",
    "        return self.input_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Method which calculates the padding based on the specified output shape and the\n",
    "# shape of the filters\n",
    "def determine_padding(filter_shape, output_shape=\"same\"):\n",
    "\n",
    "    # No padding\n",
    "    if output_shape == \"valid\":\n",
    "        return (0, 0), (0, 0)\n",
    "    # Pad so that the output shape is the same as input shape (given that stride=1)\n",
    "    elif output_shape == \"same\":\n",
    "        filter_height, filter_width = filter_shape\n",
    "\n",
    "        # Derived from:\n",
    "        # output_height = (height + pad_h - filter_height) / stride + 1\n",
    "        # In this case output_height = height and stride = 1. This gives the\n",
    "        # expression for the padding below.\n",
    "        pad_h1 = int(math.floor((filter_height - 1)/2))\n",
    "        pad_h2 = int(math.ceil((filter_height - 1)/2))\n",
    "        pad_w1 = int(math.floor((filter_width - 1)/2))\n",
    "        pad_w2 = int(math.ceil((filter_width - 1)/2))\n",
    "\n",
    "        return (pad_h1, pad_h2), (pad_w1, pad_w2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: CS231n Stanford\n",
    "def get_im2col_indices(images_shape, filter_shape, padding, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    batch_size, channels, height, width = images_shape\n",
    "    filter_height, filter_width = filter_shape\n",
    "    pad_h, pad_w = padding\n",
    "    out_height = int((height + np.sum(pad_h) - filter_height) / stride + 1)\n",
    "    out_width = int((width + np.sum(pad_w) - filter_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(filter_height), filter_width)\n",
    "    i0 = np.tile(i0, channels)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(filter_width), filter_height * channels)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(channels), filter_height * filter_width).reshape(-1, 1)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "\n",
    "# Method which turns the image shaped input to column shape.\n",
    "# Used during the forward pass.\n",
    "# Reference: CS231n Stanford\n",
    "def image_to_column(images, filter_shape, stride, output_shape='same'):\n",
    "    filter_height, filter_width = filter_shape\n",
    "\n",
    "    pad_h, pad_w = determine_padding(filter_shape, output_shape)\n",
    "\n",
    "    # Add padding to the image\n",
    "    images_padded = np.pad(images, ((0, 0), (0, 0), pad_h, pad_w), mode='constant')\n",
    "\n",
    "    # Calculate the indices where the dot products are to be applied between weights\n",
    "    # and the image\n",
    "    k, i, j = get_im2col_indices(images.shape, filter_shape, (pad_h, pad_w), stride)\n",
    "\n",
    "    # Get content from image at those indices\n",
    "    cols = images_padded[:, k, i, j]\n",
    "    channels = images.shape[1]\n",
    "    # Reshape content into column shape\n",
    "    cols = cols.transpose(1, 2, 0).reshape(filter_height * filter_width * channels, -1)\n",
    "    return cols\n",
    "\n",
    "\n",
    "\n",
    "# Method which turns the column shaped input to image shape.\n",
    "# Used during the backward pass.\n",
    "# Reference: CS231n Stanford\n",
    "def column_to_image(cols, images_shape, filter_shape, stride, output_shape='same'):\n",
    "    batch_size, channels, height, width = images_shape\n",
    "    pad_h, pad_w = determine_padding(filter_shape, output_shape)\n",
    "    height_padded = height + np.sum(pad_h)\n",
    "    width_padded = width + np.sum(pad_w)\n",
    "    images_padded = np.zeros((batch_size, channels, height_padded, width_padded))\n",
    "\n",
    "    # Calculate the indices where the dot products are applied between weights\n",
    "    # and the image\n",
    "    k, i, j = get_im2col_indices(images_shape, filter_shape, (pad_h, pad_w), stride)\n",
    "\n",
    "    cols = cols.reshape(channels * np.prod(filter_shape), -1, batch_size)\n",
    "    cols = cols.transpose(2, 0, 1)\n",
    "    # Add column content to the images at the indices\n",
    "    np.add.at(images_padded, (slice(None), k, i, j), cols)\n",
    "\n",
    "    # Return image without padding\n",
    "    return images_padded[:, :, pad_h[0]:height+pad_h[0], pad_w[0]:width+pad_w[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
