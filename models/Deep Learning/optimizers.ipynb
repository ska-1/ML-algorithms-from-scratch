{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Algorithms:\n",
    "An optimization algorithm finds the value of the parameters (weights) that minimize the error when mapping inputs to outputs. \n",
    "\n",
    "Parameters:\n",
    "- `learning_rate`: Step size for each update.\n",
    "- `momentum`: Factor to dampen oscillations.\n",
    "- `rho`: Decay rate for moving averages.\n",
    "- `eps`: Small constant for numerical stability.\n",
    "- `b1`: Decay rate for first moment estimate.\n",
    "- `b2`: Decay rate for second moment estimate.\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD):**\n",
    "     - Updates weights by moving them in the direction of the negative gradient.\n",
    "     - Incorporates momentum to maintain some velocity from previous updates.\n",
    "\n",
    "2. **Nesterov Accelerated Gradient (NAG):**\n",
    "     - Computes gradients at a \"lookahead\" position (using momentum-adjusted weights).\n",
    "     - Prevents overshooting and improves convergence.\n",
    "\n",
    "3. **Adagrad:**\n",
    "     - Accumulates squared gradients for each parameter.\n",
    "     - Adjusts learning rate dynamically for each weight, favoring sparse data by scaling down updates for frequently changing weights.\n",
    "\n",
    "4. **Adadelta:**\n",
    "     - A more robust version of Adagrad with a focus on limiting the accumulation of past gradients.\n",
    "     - Maintains running averages for gradients and updates.\n",
    "\n",
    "5. **RMSprop:**\n",
    "     - Similar to Adadelta but emphasizes smoothing gradients.\n",
    "     - Normalizes updates by the root mean square of gradients.\n",
    "\n",
    "6. **Adam (Adaptive Moment Estimation):**\n",
    "     - Combines momentum and RMSprop ideas.\n",
    "     - Maintains exponential moving averages of gradients and squared gradients.\n",
    "     - Corrects bias in initial steps to stabilize updates.\n",
    "\n",
    "### Key Features:\n",
    "- **Momentum-based Methods:** SGD and NAG.\n",
    "- **Adaptive Learning Rates:** Adagrad, Adadelta, RMSprop, and Adam.\n",
    "- **Numerical Stability:** All methods include mechanisms (like `eps`) to prevent issues like division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_diagonal(x):\n",
    "    \"\"\" Converts a vector into an diagonal matrix \"\"\"\n",
    "    m = np.zeros((len(x), len(x)))\n",
    "    for i in range(len(m[0])):\n",
    "        m[i, i] = x[i]\n",
    "    return m\n",
    "def normalize(X, axis=-1, order=2):\n",
    "    \"\"\" Normalize the dataset X \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return X / np.expand_dims(l2, axis)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers for models that use gradient based methods for finding the \n",
    "# weights that minimizes the loss.\n",
    "# A great resource for understanding these methods: \n",
    "# http://sebastianruder.com/optimizing-gradient-descent/index.html\n",
    "\n",
    "class StochasticGradientDescent():\n",
    "    def __init__(self, learning_rate=0.01, momentum=0):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.momentum = momentum\n",
    "        self.w_updt = None\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.w_updt is None:\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "        # Use momentum if set\n",
    "        self.w_updt = self.momentum * self.w_updt + (1 - self.momentum) * grad_wrt_w\n",
    "        # Move against the gradient to minimize loss\n",
    "        return w - self.learning_rate * self.w_updt\n",
    "\n",
    "class NesterovAcceleratedGradient():\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.4):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.momentum = momentum\n",
    "        self.w_updt = np.array([])\n",
    "\n",
    "    def update(self, w, grad_func):\n",
    "        # Calculate the gradient of the loss a bit further down the slope from w\n",
    "        approx_future_grad = np.clip(grad_func(w - self.momentum * self.w_updt), -1, 1)\n",
    "        # Initialize on first update\n",
    "        if not self.w_updt.any():\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "\n",
    "        self.w_updt = self.momentum * self.w_updt + self.learning_rate * approx_future_grad\n",
    "        # Move against the gradient to minimize loss\n",
    "        return w - self.w_updt\n",
    "\n",
    "class Adagrad():\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.G = None # Sum of squares of the gradients\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.G is None:\n",
    "            self.G = np.zeros(np.shape(w))\n",
    "        # Add the square of the gradient of the loss function at w\n",
    "        self.G += np.power(grad_wrt_w, 2)\n",
    "        # Adaptive gradient with higher learning rate for sparse data\n",
    "        return w - self.learning_rate * grad_wrt_w / np.sqrt(self.G + self.eps)\n",
    "\n",
    "class Adadelta():\n",
    "    def __init__(self, rho=0.95, eps=1e-6):\n",
    "        self.E_w_updt = None # Running average of squared parameter updates\n",
    "        self.E_grad = None   # Running average of the squared gradient of w\n",
    "        self.w_updt = None   # Parameter update\n",
    "        self.eps = eps\n",
    "        self.rho = rho\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.w_updt is None:\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "            self.E_w_updt = np.zeros(np.shape(w))\n",
    "            self.E_grad = np.zeros(np.shape(grad_wrt_w))\n",
    "\n",
    "        # Update average of gradients at w\n",
    "        self.E_grad = self.rho * self.E_grad + (1 - self.rho) * np.power(grad_wrt_w, 2)\n",
    "        \n",
    "        RMS_delta_w = np.sqrt(self.E_w_updt + self.eps)\n",
    "        RMS_grad = np.sqrt(self.E_grad + self.eps)\n",
    "\n",
    "        # Adaptive learning rate\n",
    "        adaptive_lr = RMS_delta_w / RMS_grad\n",
    "\n",
    "        # Calculate the update\n",
    "        self.w_updt = adaptive_lr * grad_wrt_w\n",
    "\n",
    "        # Update the running average of w updates\n",
    "        self.E_w_updt = self.rho * self.E_w_updt + (1 - self.rho) * np.power(self.w_updt, 2)\n",
    "\n",
    "        return w - self.w_updt\n",
    "\n",
    "class RMSprop():\n",
    "    def __init__(self, learning_rate=0.01, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.Eg = None # Running average of the square gradients at w\n",
    "        self.eps = 1e-8\n",
    "        self.rho = rho\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.Eg is None:\n",
    "            self.Eg = np.zeros(np.shape(grad_wrt_w))\n",
    "\n",
    "        self.Eg = self.rho * self.Eg + (1 - self.rho) * np.power(grad_wrt_w, 2)\n",
    "\n",
    "        # Divide the learning rate for a weight by a running average of the magnitudes of recent\n",
    "        # gradients for that weight\n",
    "        return w - self.learning_rate *  grad_wrt_w / np.sqrt(self.Eg + self.eps)\n",
    "\n",
    "class Adam():\n",
    "    def __init__(self, learning_rate=0.001, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 1e-8\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        # Decay rates\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros(np.shape(grad_wrt_w))\n",
    "            self.v = np.zeros(np.shape(grad_wrt_w))\n",
    "        \n",
    "        self.m = self.b1 * self.m + (1 - self.b1) * grad_wrt_w\n",
    "        self.v = self.b2 * self.v + (1 - self.b2) * np.power(grad_wrt_w, 2)\n",
    "\n",
    "        m_hat = self.m / (1 - self.b1)\n",
    "        v_hat = self.v / (1 - self.b2)\n",
    "\n",
    "        self.w_updt = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "        return w - self.w_updt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
